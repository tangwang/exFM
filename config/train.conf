# 训练数据格式，支持csv和libsvm，如果是csv格式必须确保第一行为表头（各列列名）。
data_formart     =  libsvm

# 如果是csv格式，通过csv_columns设定列名(各列列名称按","拼接)，如果未设置或者将csv_columns设为空字符串，则将读取输入文件的第一行作为列名
csv_columns = 

# 对于csv和libsvm都需要配置域分隔符和序列特征中多个值的分隔符
feat_sep         =  tab   #   default_value 	    necessary 0 helper: specify one character (or str "blank","tab") for feat_seperator in line of sample. 
feat_values_sep  =  ,    #   default_value ,     necessary 0 helper: specify one character(or str "blank","tab")  for feat_value_list_seperator in line of sample. 

# 特征配置文件路径
feat_cfg         =       #   default_value       necessary 1 helper: feature_config_path
# 如果有特征映射词典，在这里指定特征词典的KV分隔符
id_map_dict_sep        =  #default_value       necessary 0 helper: specify one character(or str "blank","tab") for k_v_seperator in line of feature mapping dict

# trainning args : 
# train data path, 如果未配置或者为空则从stdin读取
train           =       #   default_value       necessary 0 helper: trainning data path, use stdin(standard input) if not set
valid           =       #   default_value       necessary 0 helper: validation data path
# 每隔多少秒跑一次validation data
valid_interval  =  60   #   default_value 60    necessary 0 helper: how many seconds between two validition
# 如果需要热启动，在这里配置初始化模型地址
im              =       #   default_value       necessary 0 helper: init model path
# 输出模型地址
om              =       #   default_value       necessary 0 helper: output model path
# 模型格式，txt/bin
mf              =  txt  #   default_value txt   necessary 0 helper: output model format
# 训练使用的线程数
threads         =  11   #   default_value 11    necessary 0 helper: trainning threads_num


# feature id mapping settings
# 打印详细日志，0/1/2，越大打印的信息越多
verbose    =   1       #   default_value 1       necessary 0 helper: 0: only trainning logs. 1: open feature config messages, 2 : open debug messages

# option： 是否打印help message
#h                     #   option. helper: print help message

# hyper-paramerters
epoch      =   7       # train epochs
batch_size      = 1024 

# solver 支持 sgd(sgdm) / adam / adagrad / rmsprop / ftrl。不同的数据集可能最优的优化器也不一样，所以可以不同的优化器都试一下
solver = adam

# sgdm(SGD with Momentum) hyper-paramerters : (activated when solver=sgdm) :
sgdm.lr         =   0.001   #   default_value 0.001   necessary 0 helper: SGD hyper-param
sgdm.beta1      =   0.9     #   default_value 0.9     
sgdm.l1w        =   0.05    #   default_value 0.05    necessary 0  helper: l1 regularization of w
sgdm.l2w        =   1       #   default_value 5       necessary 0  helper: l2 regularization of w
sgdm.l1v        =   0.05    #   default_value 0.05    necessary 0  helper: l1 regularization of V
sgdm.l2v        =   1       #   default_value 5       necessary 0  helper: l2 regularization of V

# adagrad hyper-paramerters (activated when solver=adagrad) :
adagrad.lr = 0.002 # 0.01
adagrad.l2_norm_w = 0.1 # 1e-5
adagrad.l2_norm_V = 0.1 # 1e-5

# rmsprop hyper-paramerters (activated when solver=rmsprop) :
rmsprop.lr = 0.002 # 0.01
rmsprop.l2_norm_w = 0.1 # 1e-5
rmsprop.l2_norm_V = 0.1 # 1e-5
rmsprop.beta2 = 0.9

# adam hyper-paramerters (activated when solver=adam) :
adam.lr              =  0.0001   # defaut: 0.001  Adam learning rate
adam.beta1           =  0.9      # defaut: 0.9    adam一阶动量平滑常数
adam.beta2           =  0.999    # defaut: 0.999  adam二阶动量平滑常数
adam.weight_decay_w  =  0.1      # defaut: 2        l2正则在adam中的实现。对于adam，宜用weight_decay，不宜用l2正则。Adam通常需要比SGD更多的regularization
adam.weight_decay_V  =  0.1      # defaut: 2        同上

# FTRL hyper-paramerters (activated when solver=ftrl) :
init_stdev =   0.001   #   default_value 0.001   necessary 0 helper: stdev for initialization of 2-way factors
ftrl.w_alpha    =   0.01    #   default_value 0.01    necessary 0 helper: FTRL hyper-param
ftrl.v_alpha    =   0.01    #   default_value 0.01    necessary 0 helper: FTRL hyper-param
ftrl.w_beta     =   1       #   default_value 1       necessary 0 helper: FTRL hyper-param
ftrl.v_beta     =   1       #   default_value 1       necessary 0 helper: FTRL hyper-param
ftrl.l1w        =   0.05    #   default_value 0.05    necessary 0  helper: l1 regularization of w  
ftrl.l2w        =   5       #   default_value 5       necessary 0  helper: l2 regularization of w 
ftrl.l1v        =   0.05    #   default_value 0.05    necessary 0  helper: l1 regularization of V 
ftrl.l2v        =   5       #   default_value 5       necessary 0  helper: l2 regularization of V 
